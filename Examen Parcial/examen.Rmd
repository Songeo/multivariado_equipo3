---
title: "Examen"
output: html_document
header-includes:
 -\usepackage{amssymb}
 -\usepackage{mathtools}
 -\usepackage{lubridate}
 -\usepackage{stringr}
 -\usepackage{tidyverse}
 -\usepackage{MVA}
 
---
3c.
Partimos de usar la siguiente medida de distancias: $$ dist^2(x,y) = (x-y)^T W (x-y) $$ Con: $W$ positiva definida.

¿como cambiarian su medida de media, varianza y covarianza empirica, ¿como cambia la geometria subyacente) ?

Caso 1. Supongamos a $W$ como la matriz identidad, es decir: $$W = I_n $$ Dado que $W$ es la matriz identidad, usando la propiadad que para toda matriz $A$ el producto por la matriz identidad es igual a si misma: $$A * I_n = A$$ Podemos afirmar que $dist^2(x,y) = (x-y)^T W (x-y) = (x-y)^T(x-y)$ , sin perdida de generalidad para todo vector $x$ y $y$. El cual es simplemente la norma euclidiana.

Caso 2. Ahora supongamos a que $W$ esta compouesta por la matriz identidad multiplicada por un vector de entradas mayores a cero. $$ W°= I_n*a\quad con \quad a=[a_1 a_2 \dots a_n] \quad a_i>0 \quad \forall i=1, \dots,n $$ Lo que da como resultado que tengamos una matriz identidad reescalada en magnitud. Es decir, en la diagonal de nuestra matriz $W°$, encontramos unos o valores distintos que 1. Supongamos el individuo $i-esimo$. Supongamos que la matriz $W°$ es igual a la matriz identidad salvo constante $c$ mayor a cero en la entrada $i-esima$ de la diagonal. La distancia tomanda del individuo $i-esimo$ con todos los demás sufrira de un escalamiento dependiendo si $c<1$ o $c>1$; es decir se sobreestimara o subestimará la distancia euclidiana, según el valor de $c$. Lo que podemos concluir de este caso, es que el alterar la matriz identidad $I_n$ mediante la multiplicacion de un vector con entradas mayor a cero, es que estamos alterando la distani y esto supondria que tenemos un conocimiento previo sobre los individuos, es decir, le estamos aplicando un peso a la distancia basado en un conocimiento a priori.

3d. 
La medicion de la similitud o distancias entre dos puntos es un proceso que sirve como base para la implementacion de tecnicas de metodos multivariados. Partiendo del caso continuo, es decir, cuando nuestra matriz de individuos $X$ presenta en sus entradas datos continuos podemos pensar en aplicar metodos como el SVM, al medir las distancias entre puntos y poder general el hiperplano separador. Las tecnicas comunes para medir la distancia entre datos continuos son la distancia Euclidiana y la distancia Minkowski. Sin embargo, la medición de similitud en datos categoricos no es trivial como en los datos continuos. El problema central de los datos categoricos es que no tienen una naturaleza ordinal inherente, por lo que comparar datos categoricos directamente no es posible. La forma mas usual en encontrar similitudes entre atributos categoricos es mediante la asignación de $1$ unos en el caso de coincidencia y $0$ en el caso de no-coincidencia. La similitud entre los atributos está ligada directamente al universo de atributos en el que vive. Es apropiado pensar que el uso de la distancia $\chi ^2$ para datos categoricos tiene sentido. Recordar que: $$\tilde{\chi}^2=\frac{1}{d}\sum_{k=1}^{n} \frac{(O_k - E_k)^2}{E_k}$$ Notamos que el numerador denotado por la diferencia al cuadrado denota una comparación relativa. Podemos pensar al numerador como una diferencia entre individuos (numerador) evaluados respecto al universo en el que viven (denominador).

Ejemplos de el uso de esta distancia para datos categoricos:

$\bullet$ Ejemplo en clase, de color de ojos y pelo. En este ejemplo evaluamos que tan comun o poco comun eran las combinaciones color de ojos y color de pelo.

$\bullet$ En el trabajo de campo de la ecologia y la biologia se evaluan las las similitudes o disimilitudes entre especies. Es evidente que el estudio de estas disciplinas tiene base en muestras, ya que el analizar el universo entero de ciertas especies resultaria imposible.

4a. En un parrafo describe el objetivo (no el procedimiento) del Classical Multidimensional Scaling:

El objetivo del Multidimensional Scaling es reconstruir datos $X$ y mapear estos datos. La motivacion de este metodo se debe a que en muchas ocasiones, dado nuestro experimiento o investigacion, en vez de poder observar distancias euclidianas y medir la separacion entre los individuos de la matriz $X$, solo tenemos Similitudes o disimilitudes>. Es por eso que el proposito del metodo sera producir estas coordenadas, es decir, el MDS nos da la posibilidad de tener una representacion cartografica de algo que en si, no forma parte del plano $\Re^2$. 



4b.
Por demostrar:
Si $X$ es una matriz de datos centrada con $n$ individuos y $D^2$ es la matriz de distancias euclidianas entre $n$ individuos entonces:
$XX^T=\frac{1}{2}knD^2kn$

Sea $kn=In-\frac{1}{n}\mathbb{1}n\mathbb{1}n$

\[
In=
  \begin{pmatrix}
    1 & \cdots&  0 \\
   \cdots& 1 &\cdots  \\
    0 & \cdots & 1 \\
  \end{pmatrix}_{nxn}
\]

\[
\mathbb{1}n=
    \begin{pmatrix} 
         1\\
         1\\
         1\\
         1\\
    \end{pmatrix}_{nx1}
 \]
\[
{\mathbb{1}n}^T=
    \begin{pmatrix} 
         1
         1
         1
         1
    \end{pmatrix}_{1xn}
 \]


\[
\mathbb{1}n{\mathbb{1}n}^T=
    \begin{pmatrix} 
         1 & \cdots& 1\\
         \vdots &\ddots& \vdots\\
        1 & \cdots& 1\\
    \end{pmatrix}_{nxn}
 \]


Demostración:

$D(x_i, x_j)^2=\|x_i x_j \|^2=(x_i-x_j)^T(x_i-x_j)=x_i^Tx_i-x_i^Tx_j-x_j^Tx_i+x_j^Tx_j$

$\|x_i\|^2-2x_i^Tx_j+\|x_j\|^2$ ... (1)

Sea $W=w{\mathbb{1}n}^t$   \[w=\begin{pmatrix}\|x_1\|^2\\ \|x_2\|^2  \\\vdots\\ \|x_n\|^2 \end{pmatrix}\]

Entonces \[W=\begin{bmatrix}  \|x_1\|^2 & \|x_1\|^2 & \cdots & \|x_1\|^2 \\\|x_2\|^2 & \|x_2\|^2 & \cdots & \|x_2\|^2 \\\vdots&\vdots&\ddots&\vdots\\\ \|x_n\|^2 & \|x_n\|^2 & ... & \|x_n\|^2\end{bmatrix} \] y  \[W^T=\begin{bmatrix}  \|x_1\|^2 & \|x_2\|^2 & \cdots & \|x_n\|^2 \\\|x_1\|^2 & \|x_2\|^2 & \cdots & \|x_n\|^2 \\\vdots&\vdots&\ddots&\vdots\\\ \|x_1\|^2 & \|x_2\|^2 & ... & \|x_n\|^2\end{bmatrix} \]

Dada $W$, $D^2$ puede ser escrita como:

$D^2=W-2X^TX+W^T$...(2)

Centrando la matriz de distancias:

$knD^2kn=(knW-2knX^TX+knW^T)kn$

$=knWkn-2knX^TXkn+knW^Tkn$

$knD^2kn=-2knX^TXkn$

Notar:

$Xkn$ es la matrix $X$ centrada en columnas. 

$Xkn=\hat{X}$

$C=knD^2kn=-2\hat{X}^T\hat{X}$

$\hat{X}^T\hat{X}=-\frac{1}{2}knD^2kn$ 

4c. Usen el dataset eurodist en la librería datasets de R para reconstruir un mapa de las ciudades usando Classical MDS (pueden usar funciones o paqueterías ya hechas en R, e.g. cmdscale)Interpreta el mapa. ¿Por qué aparece rotado? Matemáticamente, ¿cuál es la causa? (Hint: invarianza bajo multiplicación por matrices ortogonales).

###Cálculo de las distancias usando la función cmdscale###
```{r}
euromat <-  as.matrix(eurodist)
euromat[1:5,1:5]

fit <- cmdscale(euromat, eig = TRUE, k = 2)
x <- fit$points[, 1]
y <- fit$points[, 2]
```

```{r}
plot(x, y, pch = 19, xlim = range(x) + c(0, 600))
text(x, y, pos = 4, labels = labels(eurodist))

```
###Usando el mismo dataset ahora calculamos las distancias ejecutando cada paso del MDS#
```{r}
dim(euromat)
d <- euromat
n <- nrow(d)
kn <- diag(1, n) - (1/n)*rep(1, n)*rep(1, n)

dim(d)
dim(kn)
b <- (-1/2) *((kn %*% d^2) %*% kn)

eigenval <- eigen(b)
c <-  diag(eigenval$values)
p <-  eigenval$vectors
fit$eig
eigenval$values

aprox <- p %*% sqrt( abs(c) ) %>% 
  as_tibble()

ggplot(aprox, aes(x = V1, y = -1*V2)) + 
  geom_point() + 
  geom_text(label = rownames(d))
```

5. Este ejercicio pretende que hagan un repaso de la idea de los métodos que expusieron sus compañeros en el curso u otros temas adicionales. Describe en una oración el objetivo de las siguientes técnicas: 

1. Correlaciones policóricas y poliseriales :
Las correlaciones policóricas y poliseriales sirven para estimar correlaciones en los casos en los que tratemos con variables ordinales, las correlaciones policóricas estiman la correlación entre variables ordinales y las poliseriales entre una variable ordinal y una variable continua. 

2. Análisis Factorial (no PCA) y Structural Equation Modeling

El análisis factorial se usa para determinar las variables que están más relacionadas con los factores más comunes y para determinar los factores necesarios para explicar los datos observados, este análisis pertenece a un marco teórico más general conocido como modelos de ecuaciones estructurales, en los cuales se busca explicar las correlaciones o covarianzas de variables observadas en función de su relación con variables latentes subyacentes. 

3. PCA dentro de una regresión lineal e interpretación de los coeficientes de regresión con diagonalización.

$\hat\beta$ es un coeficiente de regresión lineal también tenemos que  $\hat\beta = (X^TX)^{-1}X^TY$ y $\hat\Sigma$ es la matriz empírica de covarianzas.  Cada $\hat\beta_i$ es una combinación lineal de las covarianzas empíricas entre las $X_i$'s y $Y$,  ponderadas con  los elementos de $\hat\Sigma^{-1}$. Los elementos en la diagonal de la matriz de covarianzas $\hat\Sigma$ dan una medida de qué tanto se dispersan las $X_i$'s  alrededor de la media. En el caso de que no haya una correlación entre las  $X_i$'s, cada $\hat\beta_i$ representa la covarianza entre $X_i$ y $Y$ ponderada con la precisión de $X_i$. 
 También, si tenemos $PDP^T$, la eigendescomposición de $\hat\Sigma$ las columnas de $P$ son los eigenvectores de $\hat\Sigma$ y la diagonal de $D$ contiene sus eigenvalores, y las columnas de $XP$  son las observaciones observadas conocidas como componentes principales. Si transformamos $\hat\beta$ usando $P^T$ cada $\hat\beta_i$ dada por $P^T_i\hat\beta_i$ es la covarianza de la $iésima$ componente principal y $Y$  ponderada con la precisión de la $iésima$ componente  principal.

4. Item Response Theory 

Es un paradigma psicométrico utilizado en el diseño de evaluaciones, su objetivo principal es describir la probabilidad de obtener cierta respuesta en una evaluación en función de algún rasgo latente que dicha evaluación busca medir, ejemplos de estos rasgos latentes son  habilidad cognitiva, conocimiento, o alguna actitud. 

5. Multiple correspondence analysis
Es una extensión del análisis de correspondencias simples, que se aplica en el caso de tener un conjunto grande de variables categóricas. Recordemos que el análisis de correspondencias simple es una técnica conceptualmente similar al PCA pero aplicada a datos categóricos en la cual el conjunto de parámetros de chi cuadrada asociados a tablas de contingencia se descomponen en factores ortogonales.

6. Canonical correlation analysis 
Esta técnica se ocupa cuando en lugar de analizar las relaciones entre variables individuales lo que se busca es analizar la relación entre conjuntos de variables, por ejemplo en el caso de no tener una sola variable de respuesta sino un conjunto de las mismas y queremos escoger cuáles incluir en nuestro modelo.  

7.Mixed factor analysis
Es un método para tratar con datos que tengan individuos con características tanto cuantitativas como cualitativas, por lo general se aplica un análisis de PCA para las variables cuantitativas y un análisis de correspondencias múltiple (MCA) para las variables cualitativas. 

8. Canonical correlations (ver Canonical correlation analysis)